import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import random
import os

from models.model import MasterAgent  
from memory import ReplayMemory       

# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„°
STATE_DIM = 16  
ACTION_DIM = 4  
BATCH_SIZE = 32 
LR = 0.001      
GAMMA = 0.99    
SAVE_PATH = "master_agent_brain.pth"
TARGET_UPDATE_INTERVAL = 100 # [ì¶”ê°€] 100ë²ˆ í•™ìŠµë§ˆë‹¤ ì •ë‹µì§€ ë‡Œë¥¼ ë™ê¸°í™”

# 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = MasterAgent(STATE_DIM, ACTION_DIM)
target_model = MasterAgent(STATE_DIM, ACTION_DIM) # [ì¶”ê°€] ì •ë‹µì§€ ì „ìš© ë‡Œ ìƒì„±
target_model.load_state_dict(model.state_dict()) # ì‹¤ì œ ë‡Œì™€ ë˜‘ê°™ì´ ì´ˆê¸°í™”

memory = ReplayMemory(capacity=10000)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss() 

total_episodes = 0 

if os.path.exists(SAVE_PATH):
    checkpoint = torch.load(SAVE_PATH)
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        target_model.load_state_dict(checkpoint['model_state_dict']) # [ì¶”ê°€] íƒ€ê²Ÿ ë‡Œë„ ê°™ì´ ì—…ë°ì´íŠ¸
        total_episodes = checkpoint.get('total_episodes', 0)
    else:
        model.load_state_dict(checkpoint)
        target_model.load_state_dict(checkpoint)
    
    print(f"ğŸ“œ ê¸°ì¡´ ì§€ëŠ¥ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ëˆ„ì  í•™ìŠµ íšŸìˆ˜: {total_episodes}íšŒ)")
else:
    print("ğŸ£ ìƒˆë¡œìš´ ì§€ëŠ¥ìœ¼ë¡œ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

def give_reward(action, user_sentiment):
    reward = 0.0
    if action == 1 and user_sentiment == "sad":
        reward = 1.0
    return reward

def train_step():
    if len(memory) < BATCH_SIZE:
        return None
    transitions = memory.sample(BATCH_SIZE)
    batch_state = torch.FloatTensor([t[0] for t in transitions])
    batch_action = torch.LongTensor([t[1] for t in transitions]).view(-1, 1)
    batch_reward = torch.FloatTensor([t[2] for t in transitions]).view(-1, 1)
    batch_next_state = torch.FloatTensor([t[3] for t in transitions])
    batch_done = torch.FloatTensor([t[4] for t in transitions]).view(-1, 1)
    
    current_q, _ = model(batch_state)
    current_q = current_q.gather(1, batch_action)
    
    with torch.no_grad():
        # [ìˆ˜ì •] ë‹¤ìŒ ìƒíƒœì˜ ê°€ì¹˜ëŠ” target_model(ì •ë‹µì§€ ë‡Œ)ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        next_q, _ = target_model(batch_next_state)
        max_next_q = next_q.max(1)[0].view(-1, 1)
        target_q = batch_reward + (GAMMA * max_next_q * (1 - batch_done))
    
    loss = criterion(current_q, target_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # [ì¶”ê°€] ì£¼ê¸°ì ìœ¼ë¡œ ì •ë‹µì§€ ë‡Œ ì—…ë°ì´íŠ¸
    if total_episodes % TARGET_UPDATE_INTERVAL == 0:
        target_model.load_state_dict(model.state_dict())
        
    return loss.item()

def start_real_training(new_episodes=100):
    global total_episodes 
    print(f"\nğŸš€ ì¶”ê°€ í•™ìŠµ {new_episodes}íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (í˜„ì¬ ëˆ„ì : {total_episodes})")
    
    for i in range(new_episodes):
        total_episodes += 1 
        
        state = [random.uniform(-1, 1) for _ in range(STATE_DIM)] 
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            action_scores, _ = model(state_tensor)
            action = torch.argmax(action_scores).item()
        
        user_sentiment = "sad" if state[0] < 0 else "happy"
        reward = give_reward(action, user_sentiment)
        memory.push(state, action, reward, state, False)
        
        loss_val = train_step()
        
        if total_episodes % 10 == 0:
            if loss_val:
                print(f"ì—í”¼ì†Œë“œ {total_episodes} | Loss: {loss_val:.4f} ğŸ“‰")
                if total_episodes % TARGET_UPDATE_INTERVAL == 0:
                    print("ğŸ”„ [System] Target Network Synchronized.")
            else:
                print(f"ì—í”¼ì†Œë“œ {total_episodes} | ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

    save_data = {
        'model_state_dict': model.state_dict(),
        'total_episodes': total_episodes
    }
    torch.save(save_data, SAVE_PATH)
    print(f"\nğŸ’¾ ì§€ëŠ¥ê³¼ ì¹´ìš´íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ëˆ„ì : {total_episodes}íšŒ)")

if __name__ == "__main__":
    print("================================")
    print("   AI ì‹œìŠ¤í…œ ì—°ê²° ì„±ê³µ! (v1.4)   ")
    print("================================")
    
    start_real_training(100) 
    print("\nâœ… ëª¨ë“  ê³¼ì •ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")