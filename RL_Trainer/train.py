import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import random
import os

from models.model import MasterAgent  
from memory import ReplayMemory       

# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„°
STATE_DIM = 16  
ACTION_DIM = 4  
BATCH_SIZE = 32 
LR = 0.001      
GAMMA = 0.99    
SAVE_PATH = "master_agent_brain.pth"

# 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = MasterAgent(STATE_DIM, ACTION_DIM)
memory = ReplayMemory(capacity=10000)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss() 

# --- [ìˆ˜ì •] ì§€ëŠ¥ + ì¹´ìš´íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ë¡œì§ ---
total_episodes = 0 # ì´ ëˆ„ì  í•™ìŠµ íšŸìˆ˜ ì´ˆê¸°í™”

if os.path.exists(SAVE_PATH):
    checkpoint = torch.load(SAVE_PATH)
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ê²½ìš°ì™€ ì´ì „ ë²„ì „(ëª¨ë¸ë§Œ ì €ì¥ëœ ê²½ìš°)ì„ ëª¨ë‘ ëŒ€ì‘í•©ë‹ˆë‹¤.
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        total_episodes = checkpoint.get('total_episodes', 0)
    else:
        # ì´ì „ ë²„ì „ ì½”ë“œ ëŒ€ì‘ìš©
        model.load_state_dict(checkpoint)
    
    print(f"ğŸ“œ ê¸°ì¡´ ì§€ëŠ¥ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ëˆ„ì  í•™ìŠµ íšŸìˆ˜: {total_episodes}íšŒ)")
else:
    print("ğŸ£ ìƒˆë¡œìš´ ì§€ëŠ¥ìœ¼ë¡œ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
# ---------------------------------------

def give_reward(action, user_sentiment):
    reward = 0.0
    if action == 1 and user_sentiment == "sad":
        reward = 1.0
    return reward

def train_step():
    if len(memory) < BATCH_SIZE:
        return None
    transitions = memory.sample(BATCH_SIZE)
    batch_state = torch.FloatTensor([t[0] for t in transitions])
    batch_action = torch.LongTensor([t[1] for t in transitions]).view(-1, 1)
    batch_reward = torch.FloatTensor([t[2] for t in transitions]).view(-1, 1)
    batch_next_state = torch.FloatTensor([t[3] for t in transitions])
    batch_done = torch.FloatTensor([t[4] for t in transitions]).view(-1, 1)
    current_q, _ = model(batch_state)
    current_q = current_q.gather(1, batch_action)
    with torch.no_grad():
        next_q, _ = model(batch_next_state)
        max_next_q = next_q.max(1)[0].view(-1, 1)
        target_q = batch_reward + (GAMMA * max_next_q * (1 - batch_done))
    loss = criterion(current_q, target_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()

def start_real_training(new_episodes=100):
    global total_episodes # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
    print(f"\nğŸš€ ì¶”ê°€ í•™ìŠµ {new_episodes}íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (í˜„ì¬ ëˆ„ì : {total_episodes})")
    
    for i in range(new_episodes):
        total_episodes += 1 # ëˆ„ì  ì¹´ìš´íŠ¸ ì¦ê°€
        
        state = [random.uniform(-1, 1) for _ in range(STATE_DIM)] 
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            action_scores, _ = model(state_tensor)
            action = torch.argmax(action_scores).item()
        
        user_sentiment = "sad" if state[0] < 0 else "happy"
        reward = give_reward(action, user_sentiment)
        memory.push(state, action, reward, state, False)
        
        loss_val = train_step()
        
        if total_episodes % 10 == 0:
            if loss_val:
                print(f"ì—í”¼ì†Œë“œ {total_episodes} | Loss: {loss_val:.4f} ğŸ“‰")
            else:
                print(f"ì—í”¼ì†Œë“œ {total_episodes} | ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

    # ì €ì¥í•  ë•Œ ì§€ëŠ¥ê³¼ ì¹´ìš´íŠ¸ë¥¼ í•¨ê»˜ ë¬¶ì–´ì„œ ì €ì¥í•©ë‹ˆë‹¤.
    save_data = {
        'model_state_dict': model.state_dict(),
        'total_episodes': total_episodes
    }
    torch.save(save_data, SAVE_PATH)
    print(f"\nğŸ’¾ ì§€ëŠ¥ê³¼ ì¹´ìš´íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ëˆ„ì : {total_episodes}íšŒ)")

if __name__ == "__main__":
    print("================================")
    print("   AI ì‹œìŠ¤í…œ ì—°ê²° ì„±ê³µ! (v1.3)   ")
    print("================================")
    
    start_real_training(100) 
    print("\nâœ… ëª¨ë“  ê³¼ì •ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")