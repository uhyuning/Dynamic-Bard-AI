import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import random
import os
import numpy as np # [ì¶”ê°€] ìˆ˜ì¹˜ ê³„ì‚°ìš©

from models.model import MasterAgent  
# [ìˆ˜ì •] ì¼ë°˜ ReplayMemoryì—ì„œ PrioritizedMemoryë¡œ êµì²´
from memory import PrioritizedMemory       

# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„°
STATE_DIM = 16  
ACTION_DIM = 4  
BATCH_SIZE = 32 
LR = 0.001      
GAMMA = 0.99    
SAVE_PATH = "master_agent_brain.pth"
TARGET_UPDATE_INTERVAL = 100 

# 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = MasterAgent(STATE_DIM, ACTION_DIM)
target_model = MasterAgent(STATE_DIM, ACTION_DIM)
target_model.load_state_dict(model.state_dict())

# [ìˆ˜ì •] ìš°ì„ ìˆœìœ„ ë©”ëª¨ë¦¬ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”
memory = PrioritizedMemory(capacity=10000)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss() 

total_episodes = 0 

if os.path.exists(SAVE_PATH):
    checkpoint = torch.load(SAVE_PATH)
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        target_model.load_state_dict(checkpoint['model_state_dict'])
        total_episodes = checkpoint.get('total_episodes', 0)
    else:
        model.load_state_dict(checkpoint)
        target_model.load_state_dict(checkpoint)
    print(f"ğŸ“œ ê¸°ì¡´ ì§€ëŠ¥ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ëˆ„ì  í•™ìŠµ íšŸìˆ˜: {total_episodes}íšŒ)")
else:
    print("ğŸ£ ìƒˆë¡œìš´ ì§€ëŠ¥ìœ¼ë¡œ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

def give_reward(action, user_sentiment):
    reward = 0.0
    if action == 1 and user_sentiment == "sad":
        reward = 1.0
    return reward

def train_step():
    if len(memory) < BATCH_SIZE:
        return None
    
    # [ìˆ˜ì •] ìƒ˜í”Œë§ ì‹œ ì¸ë±ìŠ¤(indices)ë¥¼ í•¨ê»˜ ë°›ì•„ì˜µë‹ˆë‹¤.
    transitions, indices = memory.sample(BATCH_SIZE)
    
    batch_state = torch.FloatTensor([t[0] for t in transitions])
    batch_action = torch.LongTensor([t[1] for t in transitions]).view(-1, 1)
    batch_reward = torch.FloatTensor([t[2] for t in transitions]).view(-1, 1)
    batch_next_state = torch.FloatTensor([t[3] for t in transitions])
    batch_done = torch.FloatTensor([t[4] for t in transitions]).view(-1, 1)
    
    current_q, _ = model(batch_state)
    current_q_val = current_q.gather(1, batch_action)
    
    with torch.no_grad():
        next_q, _ = target_model(batch_next_state)
        max_next_q = next_q.max(1)[0].view(-1, 1)
        target_q = batch_reward + (GAMMA * max_next_q * (1 - batch_done))
    
    # [í•µì‹¬ ì¶”ê°€] ê° ë°ì´í„°ì˜ ì˜¤ì°¨(TD-Error) ê³„ì‚°
    # ì´ ì˜¤ì°¨ê°€ í´ìˆ˜ë¡ AIê°€ "ì˜¤! ì´ê±´ ëª°ëëŠ”ë°?" í•˜ê³  ì¤‘ìš”í•˜ê²Œ ì—¬ê¹ë‹ˆë‹¤.
    td_errors = torch.abs(current_q_val - target_q).detach().cpu().numpy()
    
    # [ìˆ˜ì •] ë©”ëª¨ë¦¬ì— ì‹¤ì œ í•™ìŠµ ì˜¤ì°¨ë¥¼ ë°˜ì˜í•˜ì—¬ ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸
    memory.update_priorities(indices, td_errors)
    
    loss = criterion(current_q_val, target_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if total_episodes % TARGET_UPDATE_INTERVAL == 0:
        target_model.load_state_dict(model.state_dict())
        
    return loss.item()

def start_real_training(new_episodes=100):
    global total_episodes 
    print(f"\nğŸš€ ì¶”ê°€ í•™ìŠµ {new_episodes}íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (í˜„ì¬ ëˆ„ì : {total_episodes})")
    
    for i in range(new_episodes):
        total_episodes += 1 
        
        state = [random.uniform(-1, 1) for _ in range(STATE_DIM)] 
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            action_scores, _ = model(state_tensor)
            action = torch.argmax(action_scores).item()
        
        user_sentiment = "sad" if state[0] < 0 else "happy"
        reward = give_reward(action, user_sentiment)
        
        # [ìˆ˜ì •] ì´ì œ ë©”ëª¨ë¦¬ê°€ ìš°ì„ ìˆœìœ„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
        memory.push(state, action, reward, state, False)
        
        loss_val = train_step()
        
        if total_episodes % 10 == 0:
            if loss_val:
                print(f"ì—í”¼ì†Œë“œ {total_episodes} | Loss: {loss_val:.4f} ğŸ“‰")
                if total_episodes % TARGET_UPDATE_INTERVAL == 0:
                    print("ğŸ”„ [System] Target Network Synchronized.")
            else:
                print(f"ì—í”¼ì†Œë“œ {total_episodes} | ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

    save_data = {
        'model_state_dict': model.state_dict(),
        'total_episodes': total_episodes
    }
    torch.save(save_data, SAVE_PATH)
    print(f"\nğŸ’¾ ì§€ëŠ¥ê³¼ ì¹´ìš´íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ëˆ„ì : {total_episodes}íšŒ)")

if __name__ == "__main__":
    print("================================")
    print("   AI ì‹œìŠ¤í…œ ì—°ê²° ì„±ê³µ! (v1.5)   ")
    print("================================")
    
    start_real_training(100) 
    print("\nâœ… ëª¨ë“  ê³¼ì •ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")