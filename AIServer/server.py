import sys
import os
from fastapi import FastAPI
import torch
import torch.optim as optim
import torch.nn.functional as F

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ë¶€í’ˆ ê°€ì ¸ì˜¤ê¸°
try:
    from Shared.protocol import EnvState, AIResponse
    from RL_Trainer.models.model import DQN  
    from RL_Trainer.utils.memory import ReplayBuffer
    from spotify_mock import SpotifyMock
    from gpt_client import GPTController
except ModuleNotFoundError as e:
    print(f"âŒ ëª¨ë“ˆ ë¡œë“œ ì—ëŸ¬: {e}. í´ë” êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
    sys.exit(1)

app = FastAPI()

# 2. AI ì„¸íŒ… (ë‡Œ, ìµœì í™” ë„êµ¬, ê¸°ì–µ ì €ì¥ì†Œ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DQN(state_dim=8, action_dim=2).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
memory = ReplayBuffer(capacity=10000)

# 3. ê°ê° ë° ì†Œí†µ ë„êµ¬ ì„¸íŒ…
spotify = SpotifyMock()
gpt = GPTController()

# [í•™ìŠµìš© ì „ì—­ ë³€ìˆ˜]
last_state = None
last_action = None

@app.get("/")
def read_root():
    return {"status": "online", "message": "ì¢…í•© AI ì—°êµ¬ì†Œ ê°€ë™ ì¤‘! ğŸ§ ğŸ¶ğŸ’¬"}

@app.post("/act", response_model=AIResponse)
async def get_action(state: EnvState):
    global last_state, last_action
    
    # [1] í˜„ì¬ ìŒì•… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    music = spotify.get_current_track_info()
    
    # [2] ë°ì´í„° ì •ë¦¬ (ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ëª¨ë“  ë°ì´í„°ë¥¼ float ìˆ«ìë¡œ ê°•ì œ ë³€í™˜)
    # ìœ„ì¹˜(3) + ì†ë„(3, ì¼ë‹¨ 0) + ìŒì•…(2) = ì´ 8ê°œ
    try:
        current_state_list = [
            float(state.position[0]), float(state.position[1]), float(state.position[2]),
            0.0, 0.0, 0.0,
            float(music['bpm'] / 160.0),
            float(music['energy'])
        ]
    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ë³€í™˜ ì—ëŸ¬: {e}")
        current_state_list = [0.0] * 8

    # [3] ê°•í™”í•™ìŠµ - ì´ì „ í–‰ë™ ë³µìŠµí•˜ê¸°
    if last_state is not None:
        reward = 1.0 if (music['energy'] > 0.5 and last_action == 0) else -0.1
        memory.push(last_state, last_action, reward, current_state_list, False)
        
        if len(memory) > 32:
            # ì‹¤ì‹œê°„ í•™ìŠµì€ ì—¬ê¸°ì„œ ë°œìƒ (ë‚˜ì¤‘ì— í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬ì²´í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)
            pass

    # [4] AIì˜ í˜„ì¬ í–‰ë™ ê²°ì •
    # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë°”ê¿€ ë•Œ [ ]ë¡œ í•œ ë²ˆ ë” ê°ì‹¸ì„œ (1, 8) ëª¨ì–‘ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    input_tensor = torch.tensor([current_state_list], dtype=torch.float32).to(device)
    
    model.eval() # íŒë‹¨ ëª¨ë“œ
    with torch.no_grad():
        prediction = model(input_tensor)
        action_idx = int(torch.argmax(prediction).item())

    # ë‹¤ìŒ ì°¨ë¡€ë¥¼ ìœ„í•´ ì €ì¥
    last_state = current_state_list
    last_action = action_idx

    # [5] í–‰ë™ ì´ë¦„ ë° GPT ì½”ë©˜íŠ¸ ìƒì„±
    action_name = "ì‹ ë‚˜ê²Œ ì „ì§„" if action_idx == 0 else "ìš°ì•„í•˜ê²Œ íšŒì „"
    ai_speech = gpt.get_ai_comment(music['genre'], music['bpm'], action_name)

    # ë¡œê·¸ ì¶œë ¥
    print(f"\n[AI ì—°êµ¬ì†Œ ë¡œê·¸]")
    print(f"ğŸµ ìŒì•…: {music['genre']} ({music['bpm']} BPM)")
    print(f"ğŸ¤– íŒë‹¨: {action_name}")
    print(f"ğŸ’¬ GPT: {ai_speech}")
    
    # [ì–¸ë¦¬ì–¼ë¡œ ìµœì¢… ì‘ë‹µ ì „ì†¡]
    return AIResponse(
        action=[float(action_idx), 0.0], 
        message=str(ai_speech)
    )

if __name__ == "__main__":
    import uvicorn
    print(f"ğŸš€ [ìµœì¢… ìˆ˜ì • ì™„ë£Œ] AI ì„œë²„ê°€ í•˜ìœ¤ì´ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤!")
    uvicorn.run(app, host="0.0.0.0", port=8000)